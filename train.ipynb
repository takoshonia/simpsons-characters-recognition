{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The \"Springfield\" Identity - Training Notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTANT: Copy dataset to local disk FIRST (run this before training!)\n",
        "# This makes training MUCH faster (10x speedup)\n",
        "import subprocess\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to find characters_train folder\n",
        "possible_paths = [\n",
        "    '/content/drive/MyDrive/shared/bonusHW/characters_train',\n",
        "    '/content/drive/MyDrive/bonusHW/characters_train',\n",
        "    '/content/drive/MyDrive/characters_train',\n",
        "    '/content/drive/MyDrive/shared/characters_train',\n",
        "]\n",
        "\n",
        "source = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        source = path\n",
        "        print(f\"Found characters_train at: {path}\")\n",
        "        break\n",
        "\n",
        "if source is None:\n",
        "    print(\"Searching for characters_train in Google Drive...\")\n",
        "    for root, dirs, files in os.walk('/content/drive/MyDrive'):\n",
        "        if 'characters_train' in dirs:\n",
        "            source = os.path.join(root, 'characters_train')\n",
        "            print(f\"Found characters_train at: {source}\")\n",
        "            break\n",
        "\n",
        "dest = '/content/characters_train'\n",
        "\n",
        "if source and os.path.exists(source):\n",
        "    if not os.path.exists(dest):\n",
        "        print(f\"Copying dataset from {source} to local disk... This may take a few minutes.\")\n",
        "        os.system(f'cp -r \"{source}\" \"{dest}\"')\n",
        "        if os.path.exists(dest):\n",
        "            print(\"✓ Dataset copied to local disk!\")\n",
        "        else:\n",
        "            print(\"✗ Copy failed. Trying alternative method...\")\n",
        "            import shutil\n",
        "            shutil.copytree(source, dest)\n",
        "            print(\"✓ Dataset copied!\")\n",
        "    else:\n",
        "        print(\"✓ Dataset already exists on local disk!\")\n",
        "else:\n",
        "    print(\"✗ characters_train folder not found!\")\n",
        "    print(\"Please make sure:\")\n",
        "    print(\"1. The folder is uploaded to Google Drive\")\n",
        "    print(\"2. Update the 'source' variable above with the correct path\")\n",
        "    print(\"\\nCurrent directory structure:\")\n",
        "    if os.path.exists('/content/drive/MyDrive'):\n",
        "        print(\"MyDrive contents:\")\n",
        "        for item in os.listdir('/content/drive/MyDrive')[:10]:\n",
        "            print(f\"  - {item}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "WORK_DIR = '/content/drive/MyDrive/bonusHW'\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(f\"\\nContents of current directory:\")\n",
        "contents = os.listdir('.') if os.path.exists('.') else []\n",
        "if contents:\n",
        "    for item in contents[:10]:\n",
        "        print(f\"  - {item}\")\n",
        "    if len(contents) > 10:\n",
        "        print(f\"  ... and {len(contents) - 10} more items\")\n",
        "else:\n",
        "    print(\"  (empty)\")\n",
        "\n",
        "if 'characters_train' in contents:\n",
        "    print(\"\\n✓ Found 'characters_train' directory!\")\n",
        "else:\n",
        "    print(\"\\n✗ 'characters_train' directory not found\")\n",
        "    print(\"\\nTo fix this:\")\n",
        "    print(\"1. Go to Google Drive (drive.google.com)\")\n",
        "    print(\"2. Navigate to: MyDrive/shared/bonusHW/\")\n",
        "    print(\"3. Upload the 'characters_train' folder here\")\n",
        "    print(\"4. Make sure it contains subdirectories like homer_simpson/, marge_simpson/, etc.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Search for characters_train in Google Drive if you're not sure where it is\n",
        "# Uncomment and run this cell to search for the folder\n",
        "\"\"\"\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def find_characters_train(start_path='/content/drive/MyDrive', max_depth=3):\n",
        "    start = Path(start_path)\n",
        "    found_paths = []\n",
        "    \n",
        "    for root, dirs, files in os.walk(start):\n",
        "        depth = root.replace(str(start), '').count(os.sep)\n",
        "        if depth > max_depth:\n",
        "            dirs.clear()\n",
        "            continue\n",
        "        \n",
        "        if 'characters_train' in dirs:\n",
        "            full_path = os.path.join(root, 'characters_train')\n",
        "            found_paths.append(full_path)\n",
        "            print(f\"Found: {full_path}\")\n",
        "    \n",
        "    return found_paths\n",
        "\n",
        "print(\"Searching for 'characters_train' folder...\")\n",
        "found = find_characters_train()\n",
        "if found:\n",
        "    print(f\"\\nFound {len(found)} location(s). You can update data_dir to use one of these paths.\")\n",
        "else:\n",
        "    print(\"\\nNot found. Please upload the characters_train folder to Google Drive.\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "os.environ['PYTHONHASHSEED'] = str(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpsonsDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_to_idx = {}\n",
        "        self.idx_to_class = {}\n",
        "        \n",
        "        character_dirs = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])\n",
        "        \n",
        "        for idx, char_dir in enumerate(character_dirs):\n",
        "            class_name = char_dir.name\n",
        "            self.class_to_idx[class_name] = idx\n",
        "            self.idx_to_class[idx] = class_name\n",
        "            \n",
        "            image_files = sorted(list(char_dir.glob('*.jpg')))\n",
        "            for img_path in image_files:\n",
        "                self.images.append(img_path)\n",
        "                self.labels.append(idx)\n",
        "        \n",
        "        print(f\"Loaded {len(self.images)} images from {len(character_dirs)} classes\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpsonsCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpsonsCNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "        self.fc1 = nn.Linear(512 * 4 * 4, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool(self.relu(self.bn5(self.conv5(x))))\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you rsynced to local disk, keep this path:\n",
        "data_dir = '/content/characters_train'\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    print(f\"ERROR: '{data_dir}' directory not found!\")\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "    print(f\"\\nPlease make sure:\")\n",
        "    print(f\"1. You ran the rsync copy cell to /content/characters_train\")\n",
        "    print(f\"   !rsync -a --info=progress2 /content/drive/MyDrive/shared/bonusHW/characters_train /content/characters_train\")\n",
        "    print(f\"2. Or update data_dir to the correct path\")\n",
        "    print(f\"\\nCurrent directory contents:\")\n",
        "    print(os.listdir('.'))\n",
        "    raise FileNotFoundError(f\"Data directory '{data_dir}' not found in {os.getcwd()}\")\n",
        "\n",
        "full_dataset = SimpsonsDataset(data_dir, transform=None)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_subset, val_subset = random_split(\n",
        "    full_dataset, \n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "class SubsetDataset(Dataset):\n",
        "    def __init__(self, base_dataset, indices, transform=None):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        actual_idx = self.indices[idx]\n",
        "        image, label = self.base_dataset[actual_idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "train_dataset = SubsetDataset(full_dataset, train_subset.indices, transform=train_transform)\n",
        "val_dataset = SubsetDataset(full_dataset, val_subset.indices, transform=val_transform)\n",
        "\n",
        "num_classes = len(full_dataset.class_to_idx)\n",
        "\n",
        "class_mappings = {\n",
        "    'class_to_idx': full_dataset.class_to_idx,\n",
        "    'idx_to_class': full_dataset.idx_to_class\n",
        "}\n",
        "\n",
        "with open(os.path.join(WORK_DIR, 'class_mappings.json'), 'w') as f:\n",
        "    json.dump(class_mappings, f)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Classes: {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 64 if torch.cuda.is_available() else 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpsonsCNN(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    \n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        model_path = os.path.join(WORK_DIR, 'model.pth')\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Model saved. Best val acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "print(f\"Training completed. Best validation accuracy: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = os.path.join(WORK_DIR, 'model.pth')\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "torch.save(model.state_dict(), model_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
